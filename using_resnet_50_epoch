{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1732825,"sourceType":"datasetVersion","datasetId":1028436}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:07:58.503413Z","iopub.execute_input":"2024-06-13T11:07:58.504121Z","iopub.status.idle":"2024-06-13T11:08:11.003174Z","shell.execute_reply.started":"2024-06-13T11:07:58.504083Z","shell.execute_reply":"2024-06-13T11:08:11.002253Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-13 11:08:00.392987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 11:08:00.393097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 11:08:00.527131: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# set the path to the image folder\npath = \"/kaggle/input/emotion-detection-fer/train\"","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:08:11.004759Z","iopub.execute_input":"2024-06-13T11:08:11.005295Z","iopub.status.idle":"2024-06-13T11:08:11.009931Z","shell.execute_reply.started":"2024-06-13T11:08:11.005266Z","shell.execute_reply":"2024-06-13T11:08:11.008750Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# define image size and batch size\nimg_size = (48,48)\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:08:11.011451Z","iopub.execute_input":"2024-06-13T11:08:11.012098Z","iopub.status.idle":"2024-06-13T11:08:11.086168Z","shell.execute_reply.started":"2024-06-13T11:08:11.012062Z","shell.execute_reply":"2024-06-13T11:08:11.085108Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# create data generator for training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:08:18.958558Z","iopub.execute_input":"2024-06-13T11:08:18.959452Z","iopub.status.idle":"2024-06-13T11:08:18.964284Z","shell.execute_reply.started":"2024-06-13T11:08:18.959414Z","shell.execute_reply":"2024-06-13T11:08:18.963168Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    directory=path,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'  # use the training subset of the data\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:08:44.152316Z","iopub.execute_input":"2024-06-13T11:08:44.153025Z","iopub.status.idle":"2024-06-13T11:09:05.383656Z","shell.execute_reply.started":"2024-06-13T11:08:44.152990Z","shell.execute_reply":"2024-06-13T11:09:05.382782Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 22968 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# create data generator for validation images\nval_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2  # split the data into training and validation sets\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:09:05.385477Z","iopub.execute_input":"2024-06-13T11:09:05.386317Z","iopub.status.idle":"2024-06-13T11:09:05.391236Z","shell.execute_reply.started":"2024-06-13T11:09:05.386280Z","shell.execute_reply":"2024-06-13T11:09:05.390158Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"val_generator = val_datagen.flow_from_directory(\n    directory=path,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'  # use the validation subset of the data\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:09:14.490279Z","iopub.execute_input":"2024-06-13T11:09:14.490670Z","iopub.status.idle":"2024-06-13T11:09:18.764814Z","shell.execute_reply.started":"2024-06-13T11:09:14.490639Z","shell.execute_reply":"2024-06-13T11:09:18.763784Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 5741 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# load the pre-trained ResNet50 model\nresnet = ResNet50(input_shape=(48,48, 3), weights='imagenet', include_top=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:09:24.640707Z","iopub.execute_input":"2024-06-13T11:09:24.641105Z","iopub.status.idle":"2024-06-13T11:09:30.180940Z","shell.execute_reply.started":"2024-06-13T11:09:24.641076Z","shell.execute_reply":"2024-06-13T11:09:30.179928Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# freeze the pre-trained layers\nfor layer in resnet.layers:\n    layer.trainable = False\n\n# add custom layers on top of the pre-trained model\nx = Flatten()(resnet.output)\nx = Dense(256, activation='relu')(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(7, activation='softmax')(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:09:36.763517Z","iopub.execute_input":"2024-06-13T11:09:36.763923Z","iopub.status.idle":"2024-06-13T11:09:36.814506Z","shell.execute_reply.started":"2024-06-13T11:09:36.763892Z","shell.execute_reply":"2024-06-13T11:09:36.813463Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# create the final model\nmodel = Model(inputs=resnet.input, outputs=x)\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:09:48.496886Z","iopub.execute_input":"2024-06-13T11:09:48.497623Z","iopub.status.idle":"2024-06-13T11:09:48.536278Z","shell.execute_reply.started":"2024-06-13T11:09:48.497588Z","shell.execute_reply":"2024-06-13T11:09:48.535265Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# train the model\nhistory = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=50\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T11:09:59.801780Z","iopub.execute_input":"2024-06-13T11:09:59.802688Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 301ms/step - accuracy: 0.2247 - loss: 1.8596 - val_accuracy: 0.2374 - val_loss: 1.8193\nEpoch 2/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 192ms/step - accuracy: 0.2580 - loss: 1.7940 - val_accuracy: 0.2698 - val_loss: 1.7725\nEpoch 3/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 195ms/step - accuracy: 0.2742 - loss: 1.7631 - val_accuracy: 0.2635 - val_loss: 1.7824\nEpoch 4/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 196ms/step - accuracy: 0.2792 - loss: 1.7436 - val_accuracy: 0.2712 - val_loss: 1.7898\nEpoch 5/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 192ms/step - accuracy: 0.2858 - loss: 1.7388 - val_accuracy: 0.3036 - val_loss: 1.7127\nEpoch 6/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.2963 - loss: 1.7099 - val_accuracy: 0.3311 - val_loss: 1.6857\nEpoch 7/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 190ms/step - accuracy: 0.3034 - loss: 1.7070 - val_accuracy: 0.3303 - val_loss: 1.6891\nEpoch 8/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3086 - loss: 1.6942 - val_accuracy: 0.3074 - val_loss: 1.6989\nEpoch 9/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.3168 - loss: 1.6900 - val_accuracy: 0.3233 - val_loss: 1.6933\nEpoch 10/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3246 - loss: 1.6860 - val_accuracy: 0.3416 - val_loss: 1.6558\nEpoch 11/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3254 - loss: 1.6817 - val_accuracy: 0.3186 - val_loss: 1.6839\nEpoch 12/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.3329 - loss: 1.6737 - val_accuracy: 0.3377 - val_loss: 1.6616\nEpoch 13/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.3365 - loss: 1.6673 - val_accuracy: 0.3325 - val_loss: 1.6443\nEpoch 14/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3409 - loss: 1.6581 - val_accuracy: 0.3402 - val_loss: 1.6403\nEpoch 15/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 190ms/step - accuracy: 0.3358 - loss: 1.6688 - val_accuracy: 0.3435 - val_loss: 1.6503\nEpoch 16/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 190ms/step - accuracy: 0.3409 - loss: 1.6551 - val_accuracy: 0.3465 - val_loss: 1.6452\nEpoch 17/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3394 - loss: 1.6585 - val_accuracy: 0.3379 - val_loss: 1.6581\nEpoch 18/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3430 - loss: 1.6638 - val_accuracy: 0.2874 - val_loss: 1.7411\nEpoch 19/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.3403 - loss: 1.6702 - val_accuracy: 0.3567 - val_loss: 1.6262\nEpoch 20/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3396 - loss: 1.6537 - val_accuracy: 0.3285 - val_loss: 1.6600\nEpoch 21/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3452 - loss: 1.6557 - val_accuracy: 0.3444 - val_loss: 1.6400\nEpoch 22/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3444 - loss: 1.6511 - val_accuracy: 0.3677 - val_loss: 1.6165\nEpoch 23/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.3472 - loss: 1.6378 - val_accuracy: 0.3074 - val_loss: 1.7189\nEpoch 24/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 191ms/step - accuracy: 0.3451 - loss: 1.6406 - val_accuracy: 0.3391 - val_loss: 1.6590\nEpoch 25/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3427 - loss: 1.6538 - val_accuracy: 0.3574 - val_loss: 1.6210\nEpoch 26/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3574 - loss: 1.6322 - val_accuracy: 0.3569 - val_loss: 1.6232\nEpoch 27/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3518 - loss: 1.6383 - val_accuracy: 0.3674 - val_loss: 1.6112\nEpoch 28/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3588 - loss: 1.6305 - val_accuracy: 0.3564 - val_loss: 1.6265\nEpoch 29/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3570 - loss: 1.6330 - val_accuracy: 0.3559 - val_loss: 1.6254\nEpoch 30/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3579 - loss: 1.6368 - val_accuracy: 0.3667 - val_loss: 1.6127\nEpoch 31/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3575 - loss: 1.6241 - val_accuracy: 0.3640 - val_loss: 1.6149\nEpoch 32/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 190ms/step - accuracy: 0.3532 - loss: 1.6331 - val_accuracy: 0.3484 - val_loss: 1.6399\nEpoch 33/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 191ms/step - accuracy: 0.3628 - loss: 1.6286 - val_accuracy: 0.3771 - val_loss: 1.6121\nEpoch 34/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 191ms/step - accuracy: 0.3610 - loss: 1.6169 - val_accuracy: 0.3740 - val_loss: 1.5929\nEpoch 35/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 190ms/step - accuracy: 0.3520 - loss: 1.6388 - val_accuracy: 0.3606 - val_loss: 1.6266\nEpoch 36/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 190ms/step - accuracy: 0.3587 - loss: 1.6237 - val_accuracy: 0.3391 - val_loss: 1.6455\nEpoch 37/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 190ms/step - accuracy: 0.3597 - loss: 1.6160 - val_accuracy: 0.3377 - val_loss: 1.6644\nEpoch 38/50\n\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 191ms/step - accuracy: 0.3535 - loss: 1.6391 - val_accuracy: 0.3515 - val_loss: 1.6442\nEpoch 39/50\n\u001b[1m355/718\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 157ms/step - accuracy: 0.3731 - loss: 1.6039","output_type":"stream"}]}]}